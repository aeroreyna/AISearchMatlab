{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metaheuristic Algorithms\n",
    "\n",
    "> “A metaheuristic is formally defined as an iterative generation process which guides a subordinate heuristic by combining intelligently different concepts for exploring and exploiting the search space, learning strategies are used to structure information in order to find efficiently near-optimal solutions.”\n",
    ">\n",
    ">@DOI:10.1007/BF02125421\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Metaheuristic algorithms are global optimizations techniques based on intelligent random search patterns, looking for either the maximum or minimum possible value of any given objective function.\n",
    "Unlike complete algorithms, heuristics are approximate algorithms which sacrifice the guarantee of finding optimal solutions for the sake of finding \"good solutions\" in a significantly reduced amount of time [@DOI:10.1145/937503.937505].\n",
    "This is specially necessary in problem with NP-hard complexity, as no polynomial time complete algorithm is known or exist (if $N\\ne NP$), thus requiring exponential computation time in the worst-case.\n",
    "\n",
    "The term _heuristic_ refers to \"to find\", and it is associated to fast and problem specific search algorithms [REF].\n",
    "This algorithms, either generate solutions from scratch by adding components, to an initially empty partial solution, until a solution is complete (constructive) or start from initial complete solutions and improve them locally until a local optima is found (local search).\n",
    "In the other hand, the term _meta_ makes reference to algorithms that are “beyond, or in an upper level” from the heuristics algorithms [REF].\n",
    "Differently from these, metaheuristics are designed to work as black boxes over any given optimization problem with none or few modifications [REF] and to avoid premature convergence, i.e., escape from local optima [REF].\n",
    "Avoiding premature convergence is achieved by either allowing worsening moves or generating new starting solutions for the local search in a more “intelligent” way than just providing random initial solutions.\n",
    "\n",
    "Most novel and popular metaheuristics on the literature are population-based [@DOI:10.1007/s10462-018-09676-2].\n",
    "These begins by generating $M$ initial random solutions as the population, with the idea to evolve them iteratively through a variety of mechanisms, usually inspired by natural phenomena.\n",
    "This initial random start makes an unbiased sample of the search space with the objective of recognize potentially \"good\" regions in the search space.\n",
    "Then, the information of the known solutions in the population and other possible data storage in memory, is used to make generate educated guesses (bias sampling), either by exploration (diversification) or exploitation (intensification).\n",
    "Exploration refers to the algorithm's ability to generate solutions in unknown regions of the search space, while exploitation is the refining of previously obtained \"good\" solutions [@DOI:10.1145/2480741.2480752].\n",
    "The skillfulness of these algorithms to find suitable solutions is mostly attributed to having the right balance between exploration and exploitation.\n",
    "However, the question of how to achieve the appropriate balance, or even measure it, is still an open issue [@DOI:10.1016/j.swevo.2019.04.008].\n",
    "For this reason, multiple metaheuristic methods are usually applied and compared in real-world problems to correctly select one that offers the best solutions for a given problem [REF].\n",
    "\n",
    "The generation of educated guesses relies on probabilistic decisions made during the search trough random variables and a variety of operators.\n",
    "Given the strong dependency of random values in these algorithms, it might seems that these will behave similarly as pure random search.\n",
    "\"The main difference to pure random search is that in metaheuristic algorithms randomness is not used blindly but in an intelligent, biased form [@Sttzle1998LocalSA].\"\n",
    "\n",
    "Given the non-deterministic nature of these algorithms, each execution travels a unique search route and returns a different final solution when the stop criteria is meet.\n",
    "This stop criteria is necessary as the value of the optimal is usually unknown and there is no guarantee that, otherwise, this can be found.\n",
    "This criteria also controls the algorithm time response, or computational complexity, as it limits the number of time the objective function is evaluated.\n",
    "\n",
    "## General Framework\n",
    "\n",
    "Population based metaheuristics can be implemented using a general framework, independently of the natural phenomenon inspiration.\n",
    "This allows their study to be focus on the mechanisms that modify the solutions in the population through the iterative process.\n",
    "\n",
    "Usually, the first step involves the definition of a set of $M$ randomly initialized solutions as follows:\n",
    "\n",
    "$$\n",
    "x_i \\in X \\mid i = 1, 2, \\ldots, M\n",
    "$${#eq:population}\n",
    "\n",
    "such that:\n",
    "\n",
    "$$\n",
    "x_i = [x_{i,1}, x_{i,2}, \\ldots, x_{i,D}] \\; | \\; x_{i,d} = R(B^{low}_d, B^{high}_d)\n",
    "$${#eq:solution_x}\n",
    "\n",
    "where $D$ is the dimensionality of the problem (number of decision variables or parameters), $x_i$ is the $i$ solution of the population $X$ and $x_{i,d} = R(B^{low}_d, B^{high}_d)$ is the uniform random assignation bounded by the target solution space (search space) for the dimension $d$.\n",
    "The solutions $x_i$ (or individuals) represent candidate solutions to be tested on the optimization problem.\n",
    "These are assigned to a corresponding quality value (or fitness) related to the objective function $f( \\cdot )$ that describes the optimization problem, such that:\n",
    "\n",
    "$$\n",
    "f_i \\in F \\mid f_{i} = f\\left( x_{i} \\right),\\; i=1,2,\\ldots,M\n",
    "$${#eq:general_FF}\n",
    "\n",
    "Using the population $X_{M\\times D}$ and fitness $F_{M\\times 1}$ information, new candidate solutions are generated by modifying currently available solutions.\n",
    "This is achieved by applying the algorithm operators (usually designed by drawing inspiration from an observed natural phenomenon).\n",
    "For most cases, this process may be illustrated by the following expression:\n",
    "\n",
    "$$\n",
    "x_{i}^{'} = x_{i} + \\mathrm{\\Delta}x_{i}\n",
    "$${#eq:change_operator}\n",
    "\n",
    "where $x_{i}^{'}$ denotes the candidate solution generated by adding up a specified update vector $\\mathrm{\\Delta} x_{i}$ to $x_{i}$.\n",
    "This update vector $\\mathrm{\\Delta}x_{i}$ depend on the specific algorithm operators.\n",
    "The generated solutions are evaluated in the fitness function to obtain their corresponding score, such as $F' = f(X')$.\n",
    "\n",
    "Finally, most nature-inspired algorithms include some kind of selection process, in which the newly generated solutions are compared against those in the current population $X^{k}$ (with $k$ denoting the current iteration) in terms of solution quality, typically with the purpose of choosing the best individual(s) among them.\n",
    "As a result of this process, a new set of solutions $X^{k + 1}\\mathbf{= \\{}x_{1}^{k + 1}\\mathbf{,}x_{2}^{k + 1}\\mathbf{,\\ldots,}x_{M}^{k + 1}\\mathbf{\\}}$, corresponding to the following iteration (or generation) '$k + 1$', is generated.\n",
    "\n",
    "This whole process is iteratively repeated until a particular stop criterion is met (i.e., a maximum number of iterations is reached).\n",
    "Once this happens, the best solution found by the algorithm $x^{*}$ is reported as the best approximation for the global optimum. The best solution is the one encounter in whole optimization process with the maximum fitness value, can be expressed as follows:\n",
    "\n",
    "$$\n",
    "f(x^{*}) \\geq f(x_i^k) \\; \\forall i, k\n",
    "$${#eq:best_sol}\n",
    "\n",
    "![General framework for population-based metaheuristics.](C:\\projects\\PhD\\Papers\\thesis\\source\\figures\\chapter4\\GeneralFramework.png){#fig:general_framework width=80%}\n",
    "\n",
    "<!-- https://whimsical.com/FhjtYRpHqPqLdXuhqMWz6f  -->\n",
    "\n",
    "The @fig:general_framework shows visually the whole general framework for the population-based metaheuristics, proposed in [@DOI:10.1007/s10462-018-09676-2] and its open source implementation [@DOI:10.5281/zenodo.3247876]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
